"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[658],{1365:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"module-4-vla/week-11-vla-models","title":"Week 11 - Vision-Language-Action (VLA) Models","description":"1. The Convergence of LLMs and Robotics","source":"@site/docs/04-module-4-vla/week-11-vla-models.md","sourceDirName":"04-module-4-vla","slug":"/module-4-vla/week-11-vla-models","permalink":"/physical-ai-textbook/docs/module-4-vla/week-11-vla-models","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/04-module-4-vla/week-11-vla-models.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Week 11 - Vision-Language-Action (VLA) Models"},"sidebar":"tutorialSidebar","previous":{"title":"Week 10 - Edge Computing with Jetson","permalink":"/physical-ai-textbook/docs/module-3-nvidia/week-10-jetson"},"next":{"title":"Week 12 - Voice-to-Action & Cognitive Planning","permalink":"/physical-ai-textbook/docs/module-4-vla/week-12-voice-action"}}');var a=t(4848),s=t(8453);const i={sidebar_position:1,title:"Week 11 - Vision-Language-Action (VLA) Models"},l="Week 11 - Vision-Language-Action (VLA) Models",r={},d=[{value:"1. The Convergence of LLMs and Robotics",id:"1-the-convergence-of-llms-and-robotics",level:2},{value:"2. How VLAs Work (e.g., RT-2)",id:"2-how-vlas-work-eg-rt-2",level:2},{value:"3. VLA Model Flow",id:"3-vla-model-flow",level:2}];function c(e){const o={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(o.header,{children:(0,a.jsx)(o.h1,{id:"week-11---vision-language-action-vla-models",children:"Week 11 - Vision-Language-Action (VLA) Models"})}),"\n",(0,a.jsx)(o.h2,{id:"1-the-convergence-of-llms-and-robotics",children:"1. The Convergence of LLMs and Robotics"}),"\n",(0,a.jsx)(o.p,{children:'Vision-Language-Action (VLA) models are the "brain" of modern robots. They combine the language understanding of Large Language Models (LLMs) with the visual perception of computer vision models. This allows the robot to understand high-level commands and execute them in the physical world.'}),"\n",(0,a.jsx)(o.h2,{id:"2-how-vlas-work-eg-rt-2",children:"2. How VLAs Work (e.g., RT-2)"}),"\n",(0,a.jsx)(o.p,{children:"Models like Google's Robotic Transformer 2 (RT-2) are trained on a massive dataset of text, images, and robot actions. This allows them to learn a direct mapping from sensory input to motor control."}),"\n",(0,a.jsx)(o.h2,{id:"3-vla-model-flow",children:"3. VLA Model Flow"}),"\n",(0,a.jsx)(o.p,{children:"The basic flow is as follows:"}),"\n",(0,a.jsx)(o.pre,{children:(0,a.jsx)(o.code,{children:'[Robot Camera Image] + "Pick up the red apple" -> [VLA Model] -> [Sequence of motor commands]\n'})}),"\n",(0,a.jsx)(o.p,{children:"The model takes in the current state of the world (via the camera) and a natural language instruction, and outputs the low-level actions required to complete the task."})]})}function u(e={}){const{wrapper:o}={...(0,s.R)(),...e.components};return o?(0,a.jsx)(o,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,o,t)=>{t.d(o,{R:()=>i,x:()=>l});var n=t(6540);const a={},s=n.createContext(a);function i(e){const o=n.useContext(s);return n.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function l(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),n.createElement(s.Provider,{value:o},e.children)}}}]);