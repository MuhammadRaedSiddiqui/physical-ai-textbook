"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[658],{1365:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>d,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/week-11-vla-models","title":"Week 11 - Vision-Language-Action (VLA) Models","description":"1. The Convergence of LLMs and Robotics","source":"@site/docs/04-module-4-vla/week-11-vla-models.md","sourceDirName":"04-module-4-vla","slug":"/module-4-vla/week-11-vla-models","permalink":"/docs/module-4-vla/week-11-vla-models","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/04-module-4-vla/week-11-vla-models.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Week 11 - Vision-Language-Action (VLA) Models"},"sidebar":"tutorialSidebar","previous":{"title":"Week 10 - Edge Computing with Jetson","permalink":"/docs/module-3-nvidia/week-10-jetson"},"next":{"title":"Week 12 - Voice-to-Action & Cognitive Planning","permalink":"/docs/module-4-vla/week-12-voice-action"}}');var i=n(4848),r=n(8453);const a={sidebar_position:1,title:"Week 11 - Vision-Language-Action (VLA) Models"},s="Week 11 - Vision-Language-Action (VLA) Models",d={},l=[{value:"1. The Convergence of LLMs and Robotics",id:"1-the-convergence-of-llms-and-robotics",level:2},{value:"2. OpenVLA: An Open Source Standard",id:"2-openvla-an-open-source-standard",level:2},{value:"3. Running OpenVLA (Python Example)",id:"3-running-openvla-python-example",level:2}];function c(e){const o={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.header,{children:(0,i.jsx)(o.h1,{id:"week-11---vision-language-action-vla-models",children:"Week 11 - Vision-Language-Action (VLA) Models"})}),"\n",(0,i.jsx)(o.h2,{id:"1-the-convergence-of-llms-and-robotics",children:"1. The Convergence of LLMs and Robotics"}),"\n",(0,i.jsx)(o.p,{children:"Vision-Language-Action (VLA) models represent a paradigm shift. Instead of separate modules for perception, planning, and control, a VLA is trained end-to-end to output robot actions (joint positions or end-effector deltas) directly from visual and textual input."}),"\n",(0,i.jsx)(o.h2,{id:"2-openvla-an-open-source-standard",children:"2. OpenVLA: An Open Source Standard"}),"\n",(0,i.jsxs)(o.p,{children:["While Google's RT-2 is proprietary, ",(0,i.jsx)(o.strong,{children:"OpenVLA"}),' is a popular open-source alternative built on Llama. It fine-tunes a Vision-Language Model (VLM) to output "action tokens" representing discretized robot movements.']}),"\n",(0,i.jsx)(o.h2,{id:"3-running-openvla-python-example",children:"3. Running OpenVLA (Python Example)"}),"\n",(0,i.jsxs)(o.p,{children:["Here is how you might load and query a VLA model using the Hugging Face ",(0,i.jsx)(o.code,{children:"transformers"})," library:"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:'from transformers import AutoModelForVision2Seq, AutoProcessor\nfrom PIL import Image\nimport torch\n\n# 1. Load Model\nmodel_id = "openvla/openvla-7b"\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModelForVision2Seq.from_pretrained(\n    model_id, \n    torch_dtype=torch.bfloat16, \n    low_cpu_mem_usage=True, \n    trust_remote_code=True\n).to("cuda")\n\n# 2. Prepare Input\nimage = Image.open("robot_view.jpg")\nprompt = "In: What action should the robot take to [pick up the red apple]?\\nOut:"\n\n# 3. Predict Action\ninputs = processor(prompt, image).to("cuda", dtype=torch.bfloat16)\naction_tokens = model.predict_action(**inputs, unnorm_key="bridge_orig")\n\nprint(f"Predicted Robot Action: {action_tokens}")\n'})})]})}function p(e={}){const{wrapper:o}={...(0,r.R)(),...e.components};return o?(0,i.jsx)(o,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,o,n)=>{n.d(o,{R:()=>a,x:()=>s});var t=n(6540);const i={},r=t.createContext(i);function a(e){const o=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function s(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:o},e.children)}}}]);