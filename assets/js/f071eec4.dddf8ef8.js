"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[90],{4378:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3-nvidia/week-09-rl-gym","title":"Week 9 - Reinforcement Learning with Isaac Gym","description":"1. Massively Parallel Simulation","source":"@site/docs/03-module-3-nvidia/week-09-rl-gym.md","sourceDirName":"03-module-3-nvidia","slug":"/module-3-nvidia/week-09-rl-gym","permalink":"/docs/module-3-nvidia/week-09-rl-gym","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/03-module-3-nvidia/week-09-rl-gym.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Week 9 - Reinforcement Learning with Isaac Gym"},"sidebar":"tutorialSidebar","previous":{"title":"Week 8 - NVIDIA Isaac Sim & SDK","permalink":"/docs/module-3-nvidia/week-08-isaac-sim"},"next":{"title":"Week 10 - Edge Computing with Jetson","permalink":"/docs/module-3-nvidia/week-10-jetson"}}');var a=n(4848),r=n(8453);const s={sidebar_position:2,title:"Week 9 - Reinforcement Learning with Isaac Gym"},o="Week 9 - Reinforcement Learning with Isaac Gym",l={},c=[{value:"1. Massively Parallel Simulation",id:"1-massively-parallel-simulation",level:2},{value:"2. Reward Functions",id:"2-reward-functions",level:2}];function d(e){const i={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"week-9---reinforcement-learning-with-isaac-gym",children:"Week 9 - Reinforcement Learning with Isaac Gym"})}),"\n",(0,a.jsx)(i.h2,{id:"1-massively-parallel-simulation",children:"1. Massively Parallel Simulation"}),"\n",(0,a.jsx)(i.p,{children:"Isaac Gym is a physics simulation environment for reinforcement learning research. Its key feature is the ability to run thousands of simulations in parallel on a single GPU. This massively accelerates the training process, allowing you to train a robot policy in hours instead of days."}),"\n",(0,a.jsx)(i.h2,{id:"2-reward-functions",children:"2. Reward Functions"}),"\n",(0,a.jsxs)(i.p,{children:["In Reinforcement Learning (RL), a ",(0,a.jsx)(i.strong,{children:"reward function"})," is a signal that tells the AI agent how well it is doing. The agent's goal is to maximize the cumulative reward."]}),"\n",(0,a.jsx)(i.p,{children:"For example, to teach a robot to walk, you might provide:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Positive reward"})," for moving forward."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Negative reward"})," for falling over."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Negative reward"})," for using too much energy."]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:"Designing a good reward function is a critical part of successful RL."})]})}function u(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>s,x:()=>o});var t=n(6540);const a={},r=t.createContext(a);function s(e){const i=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(r.Provider,{value:i},e.children)}}}]);