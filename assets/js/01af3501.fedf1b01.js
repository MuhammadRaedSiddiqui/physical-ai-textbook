"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[526],{5214:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>d,frontMatter:()=>c,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4-vla/week-12-voice-action","title":"Week 12 - Voice-to-Action & Cognitive Planning","description":"1. Voice Commands with OpenAI Whisper","source":"@site/docs/04-module-4-vla/week-12-voice-action.md","sourceDirName":"04-module-4-vla","slug":"/module-4-vla/week-12-voice-action","permalink":"/docs/module-4-vla/week-12-voice-action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/04-module-4-vla/week-12-voice-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Week 12 - Voice-to-Action & Cognitive Planning"},"sidebar":"tutorialSidebar","previous":{"title":"Week 11 - Vision-Language-Action (VLA) Models","permalink":"/docs/module-4-vla/week-11-vla-models"},"next":{"title":"Week 13 - Capstone The Autonomous Humanoid","permalink":"/docs/capstone/week-13-project"}}');var i=t(4848),a=t(8453);const c={sidebar_position:2,title:"Week 12 - Voice-to-Action & Cognitive Planning"},s="Week 12 - Voice-to-Action & Cognitive Planning",r={},l=[{value:"1. Voice Commands with OpenAI Whisper",id:"1-voice-commands-with-openai-whisper",level:2},{value:"2. Cognitive Planning with GPT-4",id:"2-cognitive-planning-with-gpt-4",level:2},{value:"3. Python Example (GPT-4 Prompt)",id:"3-python-example-gpt-4-prompt",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"week-12---voice-to-action--cognitive-planning",children:"Week 12 - Voice-to-Action & Cognitive Planning"})}),"\n",(0,i.jsx)(n.h2,{id:"1-voice-commands-with-openai-whisper",children:"1. Voice Commands with OpenAI Whisper"}),"\n",(0,i.jsx)(n.p,{children:"We can use a Speech-to-Text model like OpenAI's Whisper to convert spoken commands into text. This allows for a very natural and intuitive way to interact with a robot."}),"\n",(0,i.jsx)(n.h2,{id:"2-cognitive-planning-with-gpt-4",children:"2. Cognitive Planning with GPT-4"}),"\n",(0,i.jsx)(n.p,{children:'A high-level command like "Clean the room" is too abstract for a VLA model to handle directly. We need a "cognitive planner" to break it down into a sequence of concrete actions. We can use a powerful LLM like GPT-4 for this.'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example:"}),'\n"Clean the room" -> ',(0,i.jsx)(n.strong,{children:"GPT-4"})," ->"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\n  {"action": "find", "object": "empty_soda_can"},\n  {"action": "pick_up", "object": "empty_soda_can"},\n  {"action": "find", "object": "trash_can"},\n  {"action": "place", "object": "empty_soda_can", "target": "trash_can"},\n  {"action": "find", "object": "dirty_plate"},\n  {"action": "pick_up", "object": "dirty_plate"},\n  {"action": "find", "object": "dishwasher"},\n  {"action": "place", "object": "dirty_plate", "target": "dishwasher"}\n]\n'})}),"\n",(0,i.jsx)(n.p,{children:"This JSON sequence can then be executed one step at a time by the VLA model."}),"\n",(0,i.jsx)(n.h2,{id:"3-python-example-gpt-4-prompt",children:"3. Python Example (GPT-4 Prompt)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import openai\n\n# You would need to set up your OpenAI API key\n# openai.api_key = \'YOUR_API_KEY\'\n\ndef get_action_sequence(command: str) -> str:\n    response = openai.Completion.create(\n        engine="text-davinci-003", # Or a chat model like gpt-4\n        prompt=f"""\n        You are a cognitive planner for a home robot.\n        Convert the following high-level command into a JSON list of actions.\n        The available actions are: find, pick_up, place.\n        Command: "{command}"\n        JSON sequence:\n        """,\n        temperature=0.0,\n        max_tokens=200,\n    )\n    return response.choices[0].text\n\n# Example usage:\n# action_json = get_action_sequence("Clean up my desk")\n# print(action_json)\n'})}),"\n",(0,i.jsx)(n.p,{children:"This example shows how to use the OpenAI API to generate an action sequence from a natural language command."})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>s});var o=t(6540);const i={},a=o.createContext(i);function c(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:c(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);